# -*- coding: utf-8 -*-
"""XGBoost.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eEvXqAX4Qmd_-Z6SkCyMNT4nCqNF2wjf
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from google.colab import files
uploaded=files.upload()

df=pd.read_csv('5 th year project airlines_flights_data.csv')

df.head()

df.columns

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler, RobustScaler, PowerTransformer, QuantileTransformer, OneHotEncoder, Normalizer
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.model_selection import KFold,cross_val_score, RandomizedSearchCV
from sklearn.compose import ColumnTransformer
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
import xgboost as xgb
from sklearn.pipeline import Pipeline
import warnings
warnings.filterwarnings("ignore")

df.shape

df.columns

df.drop('index', axis=1, inplace=True)

df.shape

X = df.drop('price', axis=1)
y = df['price']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42, shuffle=True,stratify=X['airline'])

X_train.shape

X_test.shape

y_train.shape

y_test.shape

num_features = ['duration', 'days_left']
transformers = [
    StandardScaler(),
    RobustScaler(),
    QuantileTransformer(output_distribution='normal', random_state=0),
    PowerTransformer(standardize=True)
]

fig, axs = plt.subplots(len(transformers), 3, figsize=[12, 4 * len(transformers)], sharey=True)
fig.suptitle("Transformed Distributions", fontsize=12)

for i, t in enumerate(transformers):
    transformation = t
    X_train_transformed = transformation.fit_transform(X_train[num_features])
    df_transformed = pd.DataFrame(X_train_transformed, columns=num_features)
    axs[i, 0].hist(df_transformed['days_left'])
    axs[i, 0].set_xlabel('days_left')

    axs[i, 1].hist(df_transformed['duration'])
    axs[i, 1].set_xlabel('duration')
    axs[i, 1].set_title(type(transformation).__name__)

    y_train_transformed = transformation.fit_transform(y_train.values.reshape(-1, 1))  # Reshape to 2D array
    df_y_transformed = pd.DataFrame(y_train_transformed, columns=['price'])

    axs[i, 2].hist(df_y_transformed['price'])
    axs[i, 2].set_xlabel('price')
    axs[i, 2].set_title(type(transformation).__name__)
plt.tight_layout()
plt.show()

quant_transformer = QuantileTransformer(output_distribution='normal')
y_train_transformed = quant_transformer.fit_transform(y_train.values.reshape(-1, 1))
y_test_transformed = quant_transformer.transform(y_test.values.reshape(-1, 1))

quant_transformer = QuantileTransformer(output_distribution='normal')
robust_scaler = RobustScaler()
one_hot_encoder = OneHotEncoder()

categorical_features = ['index', 'airline', 'flight', 'source_city', 'departure_time', 'stops',
       'arrival_time', 'destination_city', 'class']

preprocessor = ColumnTransformer(
    transformers=[
        ('quant', quant_transformer, ['duration']),
        ('robust', robust_scaler, ['days_left']),
        ('cat', one_hot_encoder, categorical_features)
    ],
    remainder='passthrough'
)

param_grid = {
    'regressor__n_estimators': [300,400,500],
    'regressor__learning_rate': [0.5],
    'regressor__max_depth': [10, 12],
    'regressor__subsample': [0.8, 0.9],
    'regressor__colsample_bytree': [0.8, 0.9],
    'regressor__min_child_weight': [15, 20],
    'regressor__reg_alpha': [0, 0.1, 0.5],
    'regressor__reg_lambda': [0, 0.1, 0.5],
}

xgb_regressor = XGBRegressor(random_state=73)
xgb_pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('regressor', xgb_regressor)
])



xgb_search = RandomizedSearchCV(xgb_pipeline,n_iter=10, cv=5, scoring='neg_mean_squared_error',
                                random_state=42, n_jobs=-1, error_score='raise',param_distributions = param_grid)
xgb_search.fit(X_train, y_train_transformed)

best_model_xgb = xgb_search.best_estimator_
y_pred_test = best_model_xgb.predict(X_test)

# Calculate metrics
mse = mean_squared_error(y_test_transformed, y_pred_test)
rmse = np.sqrt(mse)
r2 = r2_score(y_test_transformed, y_pred_test)
mae = mean_absolute_error(y_test_transformed, y_pred_test)
print("Mean Absolute Error:", mae)
print("Mean Squared Error:", mse)
print("Root Mean Squared Error:", rmse)
print("R^2 Score:", r2)

# Train the xgb_pipeline without RandomizedSearchCV
xgb_pipeline.fit(X_train, y_train_transformed)

# Make predictions on the transformed test data
y_pred_test_direct = xgb_pipeline.predict(X_test)

# Calculate metrics using the transformed test data and predictions
mse_direct = mean_squared_error(y_test_transformed, y_pred_test_direct)
rmse_direct = np.sqrt(mse_direct)
r2_direct = r2_score(y_test_transformed, y_pred_test_direct)
mae_direct = mean_absolute_error(y_test_transformed, y_pred_test_direct)

print("Metrics without RandomizedSearchCV:")
print("Mean Absolute Error:", mae_direct)
print("Mean Squared Error:", mse_direct)
print("Root Mean Squared Error:", rmse_direct)
print("R^2 Score:", r2_direct)

# Train the xgb_pipeline without RandomizedSearchCV
xgb_pipeline.fit(X_train, y_train_transformed)

# Make predictions on the transformed test data
y_pred_test_direct = xgb_pipeline.predict(X_test)

# Calculate metrics using the transformed test data and predictions
mse_direct = mean_squared_error(y_test_transformed, y_pred_test_direct)
rmse_direct = np.sqrt(mse_direct)
r2_direct = r2_score(y_test_transformed, y_pred_test_direct)
mae_direct = mean_absolute_error(y_test_transformed, y_pred_test_direct)

print("Metrics without RandomizedSearchCV:")
print("Mean Absolute Error:", mae_direct)
print("Mean Squared Error:", mse_direct)
print("Root Mean Squared Error:", rmse_direct)
print("R^2 Score:", r2_direct)

categorical_features = ['airline', 'flight', 'source_city', 'departure_time', 'stops',
       'arrival_time', 'destination_city', 'class']

quant_transformer = QuantileTransformer(output_distribution='normal')
robust_scaler = RobustScaler()
one_hot_encoder = OneHotEncoder(handle_unknown='ignore') # Added handle_unknown to handle potential new categories in test data

preprocessor = ColumnTransformer(
    transformers=[
        ('quant', quant_transformer, ['duration']),
        ('robust', robust_scaler, ['days_left']),
        ('cat', one_hot_encoder, categorical_features)
    ],
    remainder='passthrough'
)

xgb_regressor = XGBRegressor(random_state=73)
xgb_pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('regressor', xgb_regressor)
])

# Train the xgb_pipeline without RandomizedSearchCV
xgb_pipeline.fit(X_train, y_train_transformed)

# Make predictions on the transformed test data
y_pred_test_direct = xgb_pipeline.predict(X_test)

# Calculate metrics using the transformed test data and predictions
mse_direct = mean_squared_error(y_test_transformed, y_pred_test_direct)
rmse_direct = np.sqrt(mse_direct)
r2_direct = r2_score(y_test_transformed, y_pred_test_direct)
mae_direct = mean_absolute_error(y_test_transformed, y_pred_test_direct)

print("Metrics without RandomizedSearchCV:")
print("Mean Absolute Error:", mae_direct)
print("Mean Squared Error:", mse_direct)
print("Root Mean Squared Error:", rmse_direct)
print("R^2 Score:", r2_direct)

# Get the trained XGBoost regressor from the pipeline
trained_xgb_model = xgb_pipeline.named_steps['regressor']

# Get feature importances
feature_importances = trained_xgb_model.feature_importances_

# Get the feature names after preprocessing
# The preprocessor creates new feature names for the one-hot encoded categorical features
preprocessor = xgb_pipeline.named_steps['preprocessor']
feature_names = preprocessor.get_feature_names_out()

# Create a pandas Series to easily view feature importances with their names
feature_importances_series = pd.Series(feature_importances, index=feature_names)

# Sort the feature importances in descending order
sorted_feature_importances = feature_importances_series.sort_values(ascending=False)

# Print the sorted feature importances
print("Feature Importances:")
print(sorted_feature_importances)

plt.figure(figsize=(10, 6))
plt.scatter(y_test_transformed, y_pred_test_direct, alpha=0.3, color = 'orange')
plt.xlabel("Actual Prices (Transformed)")
plt.ylabel("Predicted Prices (Transformed)")
plt.title("Actual vs. Predicted Prices (XGBoost)")
plt.grid(True)
plt.show()



print("Best parameters found by RandomizedSearchCV:")
print(xgb_search.best_params_)
print("Best cross-validation score (negative MSE):", xgb_search.best_score_)