# -*- coding: utf-8 -*-
"""Mars.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dEQeUaSPTxvUZFGiClAa8bVz4iV5CfN6
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

from google.colab import files
uploaded = files.upload()

df=pd.read_csv('5 th year project airlines_flights_data.csv')

df.columns

df.head()



from sklearn.ensemble import GradientBoostingRegressor
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

X = df.drop(columns=["price", "index"])
y = df["price"]

# Separate categorical and numeric columns
categorical_cols = ["airline", "flight", "source_city", "departure_time",
                    "stops", "arrival_time", "destination_city", "class"]
numeric_cols = ["duration", "days_left"]

# Preprocessing: One-hot encode categorical features
preprocessor = ColumnTransformer([
    ("cat", OneHotEncoder(handle_unknown="ignore"), categorical_cols),
    ("num", "passthrough", numeric_cols)
])

# Gradient Boosting Model
model = GradientBoostingRegressor(random_state=42)

# Pipeline
pipeline = Pipeline([
    ("preprocessor", preprocessor),
    ("model", model)
])

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Fit model
pipeline.fit(X_train, y_train)

# SHAP analysis
import shap
X_train_transformed = pipeline.named_steps["preprocessor"].transform(X_train).toarray()
X_test_transformed = pipeline.named_steps["preprocessor"].transform(X_test).toarray()

# Get feature names after preprocessing
feature_names = pipeline.named_steps["preprocessor"].get_feature_names_out()

explainer = shap.Explainer(pipeline.named_steps["model"], X_train_transformed)
shap_values = explainer(X_test_transformed)

# Explicitly set feature names in shap_values
shap_values.feature_names = feature_names

# Summary plot of SHAP values
shap.summary_plot(shap_values, features=X_test_transformed,
                  feature_names=feature_names)

df["route"] = df["source_city"] + " -> " + df["destination_city"]

# === Setup ===
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer, make_column_selector as selector
from sklearn.preprocessing import OneHotEncoder, SplineTransformer, StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LassoCV
from sklearn.metrics import r2_score, mean_absolute_error

# Assumes df already has your columns + a constructed 'route'
# df["route"] = df["source_city"] + " -> " + df["destination_city"]

target = "price"
cat_cols = ["airline","source_city","departure_time","stops","arrival_time",
            "destination_city","class","route"]
num_cols = ["duration","days_left"]

df_ = df.dropna(subset=cat_cols + num_cols + [target]).copy()
X = df_[cat_cols + num_cols]
y = df_[target].values

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.30, random_state=42
)

# === Column transformer ===
# - One-hot for categoricals (route/source/destination included)
# - Spline bases for numeric features to capture piecewise effects
ohe = OneHotEncoder(handle_unknown="ignore", sparse_output=False)
spl = SplineTransformer(
    degree=3,           # cubic splines
    n_knots=7,          # more knots -> more flexibility (tune)
    include_bias=False
)

pre = ColumnTransformer(
    transformers=[
        ("cat", ohe, cat_cols),
        ("spline", spl, num_cols),
    ],
    remainder="drop"
)

# === Pipeline: Spline features + standardize + LassoCV ===
# LassoCV will select the amount of shrinkage, effectively choosing useful knots/levels
pipe = Pipeline([
    ("pre", pre),
    ("scale", StandardScaler(with_mean=False)),  # features may be sparse/dense mixed
    ("model", LassoCV(cv=5, random_state=42, n_alphas=100))
])

pipe.fit(X_train, y_train)
pred = pipe.predict(X_test)
print(f"[Spline+Lasso] TEST R2: {r2_score(y_test, pred):.3f} | MAE: {mean_absolute_error(y_test, pred):.1f}")

# === Inspect grouped importances ===
# Pull feature names after transform
ohe_names = pipe.named_steps["pre"].named_transformers_["cat"].get_feature_names_out(cat_cols).tolist()
spline_names = pipe.named_steps["pre"].named_transformers_["spline"].get_feature_names_out(num_cols).tolist()
feat_names = ohe_names + spline_names

coefs = pipe.named_steps["model"].coef_
coef_s = pd.Series(coefs, index=feat_names).sort_values(key=np.abs, ascending=False)

# Group by original high-level feature (sum of |coef| within its expanded columns)
def group_sum_abs(prefix):
    cols = [c for c in coef_s.index if c.startswith(prefix + "_")]
    return float(np.abs(coef_s.loc[cols]).sum()) if cols else 0.0

print("\n[Grouped |coef| (route / source / destination / spline blocks)]")
for feat in ["route","source_city","destination_city"]:
    print(f"{feat:16s}: {group_sum_abs(feat):.4f}")

# For spline blocks (numeric piecewise effects)
for f in num_cols:
    cols = [c for c in coef_s.index if c.startswith(f + "_bspl")]
    print(f"{f:16s}: {float(np.abs(coef_s.loc[cols]).sum() if cols else 0.0):.4f}")

# === Numeric partial effects for spline terms ===
# Build a small grid to visualize the learned piecewise shape (numerical printout)
def partial_effect_numeric(feature, grid=None):
    # Hold X at typical values; vary 'feature' across a grid
    X_base = X_train.iloc[:1].copy()
    # set reference levels for categoricals
    for c in cat_cols:
        X_base[c] = X_train[c].mode()[0]
    # set medians for numeric
    for c in num_cols:
        X_base[c] = float(X_train[c].median())

    if grid is None:
        lo, hi = np.percentile(X_train[feature], [5, 95])
        grid = np.linspace(lo, hi, 9)

    rows = []
    base_pred = pipe.predict(X_base)[0]
    for v in grid:
        Xv = X_base.copy()
        Xv[feature] = v
        yv = pipe.predict(Xv)[0]
        rows.append((float(v), float(yv - base_pred)))  # centered effect
    return pd.DataFrame(rows, columns=[feature, "effect_vs_baseline"])

for f in num_cols:
    print(f"\n[Numeric partial effect via splines] {f}")
    print(partial_effect_numeric(f).to_string(index=False))

# === Route-level effects (one-hot coefficients)
# Show the top route indicators by absolute coefficient
route_coefs = coef_s[coef_s.index.str.startswith("route_")]
print("\n[Top route one-hot coefficients by |coef|]:")
print(route_coefs.head(15))

test_predictions = pipe.predict(X_test)

results_df = pd.DataFrame({'actual_price': y_test, 'predicted_price': test_predictions})
results_df['route'] = X_test['route'].reset_index(drop=True)

# Get the top N most frequent routes for plotting
top_routes = results_df['route'].value_counts().nlargest(10).index.tolist()
results_df_subset = results_df[results_df['route'].isin(top_routes)]

plt.figure(figsize=(12, 8))
sns.scatterplot(x='actual_price', y='predicted_price', hue='route', data=results_df_subset, s=20)
plt.title('Actual vs. Predicted Prices by Top 10 Routes')
plt.xlabel('Actual Price')
plt.ylabel('Predicted Price')
plt.legend(title='Route', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.grid(True)
plt.show()

# Select a subset of routes to highlight based on large absolute coefficients from the Lasso model
# We can pick the top routes from the route_coefs Series.
top_routes_by_coef = route_coefs.head(10).index.str.replace('route_', '').tolist()

# Filter the results_df for these specific routes
results_df_highlighted = results_df[results_df['route'].isin(top_routes_by_coef)].copy()

# Create faceted scatter plots for the selected routes
g = sns.FacetGrid(results_df_highlighted, col="route", col_wrap=3, height=4, aspect=1.2)
g.map(sns.scatterplot, "actual_price", "predicted_price", s=10, alpha=0.6)

# Add titles and labels
g.fig.suptitle('Actual vs. Predicted Prices for Selected Routes', y=1.02, fontsize=16)
g.set_axis_labels("Actual Price", "Predicted Price")
g.set_titles("{col_name}")
g.tight_layout()
plt.show()

plt.figure(figsize=(10, 6))
sns.boxplot(x='source_city', y='price', data=df, color='pink' )
plt.title('Price Distribution by Source City')
plt.xlabel('Source City')
plt.ylabel('Price')
plt.grid(axis='y')
plt.show()



# Generate data for partial dependence plots
duration_effect_df = partial_effect_numeric("duration")
days_left_effect_df = partial_effect_numeric("days_left")

# Plot partial dependence for duration
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
sns.lineplot(x='duration', y='effect_vs_baseline', data=duration_effect_df, color='brown')
plt.title('Partial Dependence of Price on Duration')
plt.xlabel('Duration (hours)')
plt.ylabel('Change in Predicted Price')
plt.grid(True)

# Plot partial dependence for days_left
plt.subplot(1, 2, 2)
sns.lineplot(x='days_left', y='effect_vs_baseline', data=days_left_effect_df, color='indigo')
plt.title('Partial Dependence of Price on Days Left')
plt.xlabel('Days Left Before Departure')
plt.ylabel('Change in Predicted Price')
plt.grid(True)

plt.tight_layout()
plt.show()

# Commented out IPython magic to ensure Python compatibility.
# %pip install catboost

df["route"] = df["source_city"] + " -> " + df["destination_city"]

# -------------------------------------------
# 1) Quick EDA: evidence of route effects
# -------------------------------------------
print("\nMean price by Source city:")
print(df.groupby("source_city")["price"].agg(["mean","count"]).sort_values("mean"))

print("\nMean price by Destination city:")
print(df.groupby("destination_city")["price"].agg(["mean","count"]).sort_values("mean"))

print("\nMean price by Route:")
print(df.groupby("route")["price"].agg(["mean","count"]).sort_values("mean"))

# -------------------------------------------
# 2) Train / Test split
# -------------------------------------------
from sklearn.model_selection import train_test_split
target = "price"
base_features = ["airline","source_city","departure_time","stops","arrival_time",
                 "destination_city","class","duration","days_left","route"]
X_train, X_test = train_test_split(df[base_features + [target]], test_size=0.3, random_state=42)
y_train = X_train[target].values
y_test  = X_test[target].values
X_train = X_train.drop(columns=[target])
X_test  = X_test.drop(columns=[target])

# ===========================================
# 3) MIXED EFFECTS MODEL
#    Random intercepts for Source and Destination (crossed effects)
# ===========================================
import statsmodels.formula.api as smf

# MixedLM with crossed variance components:
# - Fixed effects: duration, days_left, class (and any others you deem appropriate)
# - Random effects (variance components): source_city and destination_city
#   Using groups as a constant (variance components-only model)
tmp = pd.concat([X_train, pd.Series(y_train, name=target)], axis=1).copy()
tmp["ones"] = 1  # constant grouping to enable vc_formula-only random effects

# Drop rows with missing values before fitting the model
tmp = tmp.dropna()

# Manually create dummy variables for 'class' and add them to tmp
class_dummies = pd.get_dummies(tmp['class'], prefix='class', drop_first=True)
# Corrected line: do not concatenate source_city and destination_city back
tmp = pd.concat([tmp, class_dummies], axis=1)

vc = {"source_city": "0 + C(source_city)", "destination_city": "0 + C(destination_city)"}
# Update formula to use the dummy variables instead of C(class)
# Only add class dummies to the formula if there are any
formula = "price ~ duration + days_left"
if not class_dummies.empty:
    formula += " + " + " + ".join(class_dummies.columns) # extend with other fixed effects if desired


# Fit on TRAIN
mixed = smf.mixedlm(formula, data=tmp, groups=tmp["ones"], vc_formula=vc)
mixed_res = mixed.fit(reml=False, method="lbfgs")
print("\n[MIXED] Summary (TRAIN):")
print(mixed_res.summary())

# Explain variance decomposition: how much variability sits at Source vs Destination vs Residual
var_comp = mixed_res.vcomp  # variance components
# Get variance component names from vc_formula keys and the residual
vc_names = list(vc.keys()) + ['Residual']
print("\n[MIXED] Variance components:")
for n, v in zip(vc_names, var_comp):
    print(f"{n}: {v:.4f}")
print(f"Residual variance: {mixed_res.scale:.4f}")

# Predict on TEST
tmp_test = pd.concat([X_test, pd.Series(y_test, name=target)], axis=1).copy()
tmp_test["ones"] = 1

# Manually create dummy variables for 'class' in tmp_test as well
class_dummies_test = pd.get_dummies(tmp_test['class'], prefix='class', drop_first=True)
tmp_test = pd.concat([tmp_test, class_dummies_test], axis=1)

# Drop rows with missing values from tmp_test and align y_test
tmp_test = tmp_test.dropna()
# Align y_test to the filtered tmp_test index
y_test_aligned = y_test[tmp_test.index]


y_pred_mixed = mixed_res.predict(tmp_test)
from sklearn.metrics import r2_score, mean_absolute_error
print(f"\n[MIXED] TEST R2: {r2_score(y_test_aligned, y_pred_mixed):.3f} | MAE: {mean_absolute_error(y_test_aligned, y_pred_mixed):.1f}")

# ===========================================
# 4) TREE MODELS (CatBoost / LightGBM) + SHAP
# ===========================================
cat_cols = ["airline","source_city","departure_time","stops","arrival_time",
            "destination_city","class","route"]
num_cols = ["duration","days_left"]

# ---- 4A) CatBoost (native categorical handling) ----
try:
    from catboost import CatBoostRegressor, Pool
    Xtr = X_train[cat_cols + num_cols].copy()
    Xte = X_test[cat_cols + num_cols].copy()
    ytr, yte = y_train, y_test

    cat_idx = [Xtr.columns.get_loc(c) for c in cat_cols]
    train_pool = Pool(Xtr, label=ytr, cat_features=cat_idx)
    test_pool  = Pool(Xte, label=yte, cat_features=cat_idx)

    cb = CatBoostRegressor(
        depth=6, learning_rate=0.05, n_estimators=800,
        loss_function="RMSE", random_state=42, verbose=False
    )
    cb.fit(train_pool, eval_set=test_pool, verbose=False)
    pred_cb = cb.predict(test_pool)
    print(f"\n[CatBoost] TEST R2: {r2_score(yte, pred_cb):.3f} | MAE: {mean_absolute_error(yte, pred_cb):.1f}")

    # SHAP importance for route / source / destination
    try:
        import shap
        explainer = shap.TreeExplainer(cb)
        shap_vals = explainer.shap_values(Xte)
        global_imp = pd.Series(np.abs(shap_vals).mean(0), index=Xte.columns).sort_values(ascending=False)
        print("\n[CatBoost] Mean |SHAP| (top features):")
        print(global_imp.head(10))
        print("\n[CatBoost] Route vs. Source/Destination contributions:")
        print(global_imp.loc[["route","source_city","destination_city"]])
        # Optional plotting locally:
        # shap.summary_plot(shap_vals, Xte)
        # shap.dependence_plot("route", shap_vals, Xte, interaction_index=None)
    except Exception as e:
        print(f"[CatBoost] SHAP skipped: {e}")
except Exception as e:
    print(f"[CatBoost] skipped: {e}")


# ---- 4B) LightGBM (fast GBDT) ----
try:
    import lightgbm as lgb
    Xtr = X_train[cat_cols + num_cols].copy()
    Xte = X_test[cat_cols + num_cols].copy()
    ytr, yte = y_train, y_test

    for c in cat_cols:
        Xtr[c] = Xtr[c].astype("category")
        Xte[c] = Xte[c].astype("category")

    lgbm = lgb.LGBMRegressor(
        n_estimators=1000, learning_rate=0.05, num_leaves=31, random_state=42
    )
    # Removed verbose argument as it caused an error
    lgbm.fit(Xtr, ytr, eval_set=[(Xte, yte)], eval_metric="l2")
    pred_lgbm = lgbm.predict(Xte)
    print(f"\n[LightGBM] TEST R2: {r2_score(yte, pred_lgbm):.3f} | MAE: {mean_absolute_error(yte, pred_lgbm):.1f}")

    # SHAP (global importance focus)
    try:
        import shap
        explainer = shap.TreeExplainer(lgbm)
        shap_vals = explainer.shap_values(Xte)
        global_imp = pd.Series(np.abs(shap_vals).mean(0), index=Xte.columns).sort_values(ascending=False)
        print("\n[LightGBM] Mean |SHAP| (top features):")
        print(global_imp.head(10))
        print("\n[LightGBM] Route vs. Source/Destination contributions:")
        print(global_imp.loc[["route","source_city","destination_city"]])
    except Exception as e:
        print(f"[LightGBM] SHAP skipped: {e}")
except Exception as e:
    print(f"[LightGBM] skipped: {e}")


# ===========================================
# 5) MARS (Earth): piecewise effects of route/cities
# ===========================================
# Earth requires numeric inputs -> one-hot encode categoricals, keep numeric as-is.
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.pipeline import Pipeline
from sklearn.metrics import r2_score, mean_absolute_error
try:
    from pyearth import Earth  # sklearn-contrib-py-earth

    ohe = OneHotEncoder(handle_unknown="ignore", sparse_output=False)
    pre = ColumnTransformer([
        ("ohe", ohe, cat_cols),           # one-hot for categorical: includes route/source/destination
        ("pass", "passthrough", num_cols) # duration, days_left
    ])

    mars = Pipeline([
        ("pre", pre),
        ("earth", Earth(max_terms=50, max_degree=2))  # allow interactions up to 2
    ])
    mars.fit(X_train[cat_cols + num_cols], y_train)
    pred_mars = mars.predict(X_test[cat_cols + num_cols])
    print(f"\n[MARS] TEST R2: {r2_score(y_test, pred_mars):.3f} | MAE: {mean_absolute_error(y_test, pred_mars):.1f}")

    # Variable importance (grouped for route / source / destination)
    # Get names after one-hot:
    ohe_names = mars.named_steps["pre"].named_transformers_["ohe"].get_feature_names_out(cat_cols).tolist()
    feature_names = ohe_names + num_cols
    importances = mars.named_steps["earth"].feature_importances_
    imp_series = pd.Series(importances, index=feature_names).sort_values(ascending=False)

    # Group OHE columns back to original features
    def group_imp(prefix):
        cols = [c for c in imp_series.index if c.startswith(prefix + "_")]
        return float(imp_series.loc[cols].sum()) if cols else 0.0

    print("\n[MARS] Grouped importances (sum over one-hot columns):")
    for feat in ["route","source_city","destination_city","duration","days_left"]:
        print(f"{feat}: {group_imp(feat):.4f}")

except Exception as e:
    print(f"[MARS] skipped: {e}")


# ===========================================
# 6) Practical interpretation helpers
# ===========================================
print("\nINTERPRETATION TIPS:")
print("- Mixed Effects: compare variance components for source_city and destination_city; larger values imply")
print("  stronger city-specific price variability. Use the fixed-effect coefficients for duration/days_left/class controls.")
print("- CatBoost/LightGBM: if SHAP |route| (or source/destination) is large vs. other features, price materially changes by route/cities.")
print("- MARS: grouped importances for route/source/destination show piecewise impact; inspect which routes carry bigger effects.")

plt.figure(figsize=(10, 6))
plt.scatter(yte, pred_lgbm, alpha=0.5, color='g')
plt.title('Actual vs. Predicted Prices (LightGBM)')
plt.xlabel('Actual Price')
plt.ylabel('Predicted Price')
plt.grid(True)
plt.plot([yte.min(), yte.max()], [yte.min(), yte.max()], 'k--', lw=2) # Add a diagonal line for reference
plt.show()

residuals_lgbm = yte - pred_lgbm

plt.figure(figsize=(10, 6))
plt.scatter(pred_lgbm, residuals_lgbm, alpha=0.5, color='y')
plt.title('Residual Plot (LightGBM)')
plt.xlabel('Predicted Price')
plt.ylabel('Residuals (Actual - Predicted)')
plt.axhline(y=0, color='r', linestyle='--') # Add a horizontal line at y=0
plt.grid(True)
plt.show()

# Assuming 'pred_cb' and 'yte' are available from the CatBoost execution

plt.figure(figsize=(10, 6))
plt.scatter(yte, pred_cb, alpha=0.5, color ='purple')
plt.title('Actual vs. Predicted Prices (CatBoost)')
plt.xlabel('Actual Price')
plt.ylabel('Predicted Price')
plt.grid(True)
plt.plot([yte.min(), yte.max()], [yte.min(), yte.max()], 'k--', lw=2) # Add a diagonal line for reference
plt.show()

# Assuming 'pred_cb' and 'yte' are available from the CatBoost execution

residuals_cb = yte - pred_cb

plt.figure(figsize=(10, 6))
plt.scatter(pred_cb, residuals_cb, alpha=0.5, color='r')
plt.title('Residual Plot (CatBoost)')
plt.xlabel('Predicted Price')
plt.ylabel('Residuals (Actual - Predicted)')
plt.axhline(y=0, color='r', linestyle='--') # Add a horizontal line at y=0
plt.grid(True)
plt.show()

import shap
# Assuming 'cb' (CatBoost model) and 'Xte' are available from the CatBoost execution

# Re-calculate SHAP values for CatBoost if not already done or if needed
# explainer_cb = shap.TreeExplainer(cb)
# shap_vals_cb = explainer_cb.shap_values(Xte)

# Assuming shap_vals is the SHAP values from the last tree model execution (CatBoost)
# If not, explicitly use shap_vals_cb if you re-calculated them above

# Need to ensure the correct SHAP values are used if multiple tree models were run in the same cell
# Based on the previous execution, the shap_vals variable holds the SHAP values for LightGBM.
# We need to re-run the SHAP calculation specifically for the CatBoost model.

try:
    explainer_cb = shap.TreeExplainer(cb)
    shap_vals_cb = explainer_cb.shap_values(Xte)

    plt.figure(figsize=(10, 8))
    shap.summary_plot(shap_vals_cb, Xte, feature_names=Xte.columns)
except Exception as e:
    print(f"SHAP plot for CatBoost skipped: {e}")

print("INTERPRETATION TIPS:")
print("- Mixed Effects: compare variance components for source_city and destination_city; larger values imply")
print("  stronger city-specific price variability. Use the fixed-effect coefficients for duration/days_left/class controls.")
print("- CatBoost/LightGBM: if SHAP |route| (or source/destination) is large vs. other features, price materially changes by route/cities.")
print("- MARS: grouped importances for route/source/destination show piecewise impact; inspect which routes carry bigger effects.")