# -*- coding: utf-8 -*-
"""2way Anova.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Yai8qDsxeH2AljLC_vmU_5_5Yn_Obg83
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

from google.colab import files
uploaded=files.upload()

df=pd.read_csv('airlines_flights_data.csv')

df.shape

df.columns



from sklearn.ensemble import GradientBoostingRegressor
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

"""GBM"""

X = df.drop(columns=["price", "index"])
y = df["price"]

# Separate categorical and numeric columns
categorical_cols = ["airline", "flight", "source_city", "departure_time",
                    "stops", "arrival_time", "destination_city", "class"]
numeric_cols = ["duration", "days_left"]

# Preprocessing: One-hot encode categorical features
preprocessor = ColumnTransformer([
    ("cat", OneHotEncoder(handle_unknown="ignore"), categorical_cols),
    ("num", "passthrough", numeric_cols)
])

# Gradient Boosting Model
model = GradientBoostingRegressor(random_state=42)

# Pipeline
pipeline = Pipeline([
    ("preprocessor", preprocessor),
    ("model", model)
])

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Fit model
pipeline.fit(X_train, y_train)

# SHAP analysis
import shap
X_train_transformed = pipeline.named_steps["preprocessor"].transform(X_train).toarray()
X_test_transformed = pipeline.named_steps["preprocessor"].transform(X_test).toarray()

# Get feature names after preprocessing
feature_names = pipeline.named_steps["preprocessor"].get_feature_names_out()

explainer = shap.Explainer(pipeline.named_steps["model"], X_train_transformed)
shap_values = explainer(X_test_transformed)

# Explicitly set feature names in shap_values
shap_values.feature_names = feature_names

# Summary plot of SHAP values
shap.summary_plot(shap_values, features=X_test_transformed,
                  feature_names=feature_names)

"""2 WAY ANOVA"""

import statsmodels.api as sm
from statsmodels.formula.api import ols

# Two-way ANOVA model with interaction
model = ols("price ~ C(source_city) * C(destination_city)", data=df).fit()
anova_table = sm.stats.anova_lm(model, typ=2)  # Type II ANOVA table
print(anova_table)

"""RANDOM FOREST REGRESSOR"""

features = ["airline","source_city","departure_time","stops","arrival_time",
            "destination_city","class","duration","days_left"]
target = "price"

# Basic sanity check
df = df.dropna(subset=features+[target]).copy()

# -------------------------------
# 1) Quick EDA (evidence without modeling)
# -------------------------------
print("\nMean price by departure_time:")
print(df.groupby("departure_time")[target].mean().sort_values())

print("\nMean price by arrival_time:")
print(df.groupby("arrival_time")[target].mean().sort_values())

# -------------------------------
# 2) Train/Test split
# -------------------------------
from sklearn.model_selection import train_test_split
X_train, X_test = train_test_split(df, test_size=0.3, random_state=42)

cat_cols = ["airline","source_city","departure_time","stops","arrival_time",
            "destination_city","class"]
num_cols = ["duration","days_left"]



# -------------------------------
# Random Forest + One-Hot + SHAP
# -------------------------------
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, mean_absolute_error

ohe = OneHotEncoder(handle_unknown="ignore", sparse_output=False)
pre = ColumnTransformer([
    ("ohe", ohe, cat_cols),
    ("passthrough", "passthrough", num_cols),
])

rf = Pipeline([
    ("pre", pre),
    ("model", RandomForestRegressor(
        n_estimators=600, min_samples_leaf=2, random_state=42, n_jobs=-1
    ))
])
rf.fit(X_train[cat_cols+num_cols], X_train[target])
rf_pred = rf.predict(X_test[cat_cols+num_cols])
print(f"\n[RandomForest] R2: {r2_score(X_test[target], rf_pred):.3f} | MAE: {mean_absolute_error(X_test[target], rf_pred):.1f}")

# SHAP with RF (on the transformed design matrix)
try:
    import shap
    # Build a small wrapper to transform X_test
    Xte_design = rf.named_steps["pre"].transform(X_test[cat_cols+num_cols])
    rf_model = rf.named_steps["model"]
    explainer = shap.TreeExplainer(rf_model)
    shap_vals = explainer.shap_values(Xte_design)

    # Map OHE back to original features by grouping columns
    ohe_feat_names = rf.named_steps["pre"].named_transformers_["ohe"].get_feature_names_out(cat_cols)
    design_cols = list(ohe_feat_names) + num_cols
    shap_df = pd.DataFrame(np.abs(shap_vals), columns=design_cols)
    # Group by original feature (sum absolute SHAP across its one-hot levels)
    def group_importance(prefix):
        cols = [c for c in shap_df.columns if c.startswith(prefix + "_")]
        return shap_df[cols].values.mean() if cols else np.nan

    dep_imp = group_importance("departure_time")
    arr_imp = group_importance("arrival_time")
    print("\n[RandomForest] Mean |SHAP| (grouped) for departure_time and arrival_time:")
    print(f"departure_time: {dep_imp:.4f}")
    print(f"arrival_time  : {arr_imp:.4f}")

except Exception as e:
    print(f"[RandomForest] SHAP skipped: {e}")

# -------------------------------
# 3D) GAM for smooth, interpretable effects
# -------------------------------
# Encode departure/arrival time as ordered categories if desired; here, treat as categorical via one-hot.
try:
    from pygam import LinearGAM, s, f
    # For GAM, build a simple pandas -> design approach:
    # Convert categoricals to category dtype to use f() factors
    Xg = df[cat_cols+num_cols].copy()
    yg = df[target].values
    for c in cat_cols:
        Xg[c] = Xg[c].astype("category")

    # Build terms: factors for departure_time & arrival_time; s() for numeric smooths
    # Note: pygam works with numpy arrays; use pandas codes for factors
    def factor_codes(series):
        return series.cat.codes.values

    Xgam = np.column_stack([
        factor_codes(Xg["departure_time"]),
        factor_codes(Xg["arrival_time"]),
        Xg["duration"].values,
        Xg["days_left"].values
    ])

    # Indices: 0=dep_time (factor), 1=arr_time (factor), 2=duration (smooth), 3=days_left (smooth)
    # Build a GAM with factor terms (f()) + smooth terms (s())
    gam = LinearGAM( f(0) + f(1) + s(2) + s(3) ).fit(Xgam, yg)
    print("\n[GAM] pseudo-R2:", gam.statistics_['pseudo_r2'])

    # Inspect factor effects (depart/arr): larger deviations imply price differences by time bins
    # Partial dependence for factor levels (print mean effect per level)
    def factor_effects(term_index):
        te = gam.partial_dependence(term=term_index, X=Xgam)
        # Group by integer level
        levels = np.unique(Xgam[:, term_index].astype(int))
        means = {int(l): float(np.mean(te[Xgam[:, term_index]==l])) for l in levels}
        return means

    dep_effects = factor_effects(0)
    arr_effects = factor_effects(1)
    print("\n[GAM] Departure_time factor partial effects (by encoded level):", dep_effects)
    print("[GAM] Arrival_time   factor partial effects (by encoded level):", arr_effects)
except Exception as e:
    print(f"[GAM] skipped: {e}")

# -------------------------------
# 4) Minimal conclusion helpers
# -------------------------------
print("\nINTERPRETATION TIPS:")
print("- If SHAP/grouped-SHAP for departure_time/arrival_time is non-trivial vs other features,")
print("  then price changes with those time-of-day bins.")
print("- In GAM, factor partial effects differing meaningfully across levels imply price shifts by time bins.")

!pip install catboost
!pip install sklearn-contrib-py-earth

"""CATBOOST REGRESSOR + LIGHT GBM + SHAP"""

df["route"] = df["source_city"] + " -> " + df["destination_city"]

# -------------------------------------------
# 1) Quick EDA: evidence of route effects
# -------------------------------------------
print("\nMean price by Source city:")
print(df.groupby("source_city")["price"].agg(["mean","count"]).sort_values("mean"))

print("\nMean price by Destination city:")
print(df.groupby("destination_city")["price"].agg(["mean","count"]).sort_values("mean"))

print("\nMean price by Route:")
print(df.groupby("route")["price"].agg(["mean","count"]).sort_values("mean"))

# -------------------------------------------
# 2) Train / Test split
# -------------------------------------------
from sklearn.model_selection import train_test_split
target = "price"
base_features = ["airline","source_city","departure_time","stops","arrival_time",
                 "destination_city","class","duration","days_left","route"]
X_train, X_test = train_test_split(df[base_features + [target]], test_size=0.3, random_state=42)
y_train = X_train[target].values
y_test  = X_test[target].values
X_train = X_train.drop(columns=[target])
X_test  = X_test.drop(columns=[target])

# ===========================================
# 3) MIXED EFFECTS MODEL
#    Random intercepts for Source and Destination (crossed effects)
# ===========================================
import statsmodels.formula.api as smf

# MixedLM with crossed variance components:
# - Fixed effects: duration, days_left, class (and any others you deem appropriate)
# - Random effects (variance components): source_city and destination_city
#   Using groups as a constant (variance components-only model)
tmp = pd.concat([X_train, pd.Series(y_train, name=target)], axis=1).copy()
tmp["ones"] = 1  # constant grouping to enable vc_formula-only random effects

# Drop rows with missing values before fitting the model
tmp = tmp.dropna()

# Manually create dummy variables for 'class' and add them to tmp
class_dummies = pd.get_dummies(tmp['class'], prefix='class', drop_first=True)
tmp = pd.concat([tmp, class_dummies], axis=1)

vc = {"source_city": "0 + C(source_city)", "destination_city": "0 + C(destination_city)"}
# Update formula to use the dummy variables instead of C(class)
# Only add class dummies to the formula if there are any
formula = "price ~ duration + days_left"
if not class_dummies.empty:
    formula += " + " + " + ".join(class_dummies.columns) # extend with other fixed effects if desired


# Fit on TRAIN
mixed = smf.mixedlm(formula, data=tmp, groups=tmp["ones"], vc_formula=vc)
mixed_res = mixed.fit(reml=False, method="lbfgs")
print("\n[MIXED] Summary (TRAIN):")
print(mixed_res.summary())

# Explain variance decomposition: how much variability sits at Source vs Destination vs Residual
var_comp = mixed_res.vcomp  # variance components
# Get variance component names from vc_formula keys and the residual
vc_names = list(vc.keys()) + ['Residual']
print("\n[MIXED] Variance components:")
for n, v in zip(vc_names, var_comp):
    print(f"{n}: {v:.4f}")
print(f"Residual variance: {mixed_res.scale:.4f}")

# Predict on TEST
tmp_test = pd.concat([X_test, pd.Series(y_test, name=target)], axis=1).copy()
tmp_test["ones"] = 1

# Manually create dummy variables for 'class' in tmp_test as well
class_dummies_test = pd.get_dummies(tmp_test['class'], prefix='class', drop_first=True)
tmp_test = pd.concat([tmp_test, class_dummies_test], axis=1)

# Drop rows with missing values from tmp_test and align y_test
tmp_test = tmp_test.dropna()
y_test_aligned = y_test[tmp_test.index]


y_pred_mixed = mixed_res.predict(tmp_test)
from sklearn.metrics import r2_score, mean_absolute_error
print(f"\n[MIXED] TEST R2: {r2_score(y_test_aligned, y_pred_mixed):.3f} | MAE: {mean_absolute_error(y_test_aligned, y_pred_mixed):.1f}")

# ===========================================
# 4) TREE MODELS (CatBoost / LightGBM) + SHAP
# ===========================================
cat_cols = ["airline","source_city","departure_time","stops","arrival_time",
            "destination_city","class","route"]
num_cols = ["duration","days_left"]

# ---- 4A) CatBoost (native categorical handling) ----
try:
    from catboost import CatBoostRegressor, Pool
    Xtr = X_train[cat_cols + num_cols].copy()
    Xte = X_test[cat_cols + num_cols].copy()
    ytr, yte = y_train, y_test

    cat_idx = [Xtr.columns.get_loc(c) for c in cat_cols]
    train_pool = Pool(Xtr, label=ytr, cat_features=cat_idx)
    test_pool  = Pool(Xte, label=yte, cat_features=cat_idx)

    cb = CatBoostRegressor(
        depth=6, learning_rate=0.05, n_estimators=800,
        loss_function="RMSE", random_state=42, verbose=False
    )
    cb.fit(train_pool, eval_set=test_pool, verbose=False)
    pred_cb = cb.predict(test_pool)
    print(f"\n[CatBoost] TEST R2: {r2_score(yte, pred_cb):.3f} | MAE: {mean_absolute_error(yte, pred_cb):.1f}")

    # SHAP importance for route / source / destination
    try:
        import shap
        explainer = shap.TreeExplainer(cb)
        shap_vals = explainer.shap_values(Xte)
        global_imp = pd.Series(np.abs(shap_vals).mean(0), index=Xte.columns).sort_values(ascending=False)
        print("\n[CatBoost] Mean |SHAP| (top features):")
        print(global_imp.head(10))
        print("\n[CatBoost] Route vs. Source/Destination contributions:")
        print(global_imp.loc[["route","source_city","destination_city"]])
        # Optional plotting locally:
        # shap.summary_plot(shap_vals, Xte)
        # shap.dependence_plot("route", shap_vals, Xte, interaction_index=None)
    except Exception as e:
        print(f"[CatBoost] SHAP skipped: {e}")
except Exception as e:
    print(f"[CatBoost] skipped: {e}")

# ---- 4B) LightGBM (fast GBDT) ----
try:
    import lightgbm as lgb
    Xtr = X_train[cat_cols + num_cols].copy()
    Xte = X_test[cat_cols + num_cols].copy()
    ytr, yte = y_train, y_test

    for c in cat_cols:
        Xtr[c] = Xtr[c].astype("category")
        Xte[c] = Xte[c].astype("category")

    lgbm = lgb.LGBMRegressor(
        n_estimators=1000, learning_rate=0.05, num_leaves=31, random_state=42
    )
    lgbm.fit(Xtr, ytr, eval_set=[(Xte, yte)], eval_metric="l2", verbose=False)
    pred_lgbm = lgbm.predict(Xte)
    print(f"\n[LightGBM] TEST R2: {r2_score(yte, pred_lgbm):.3f} | MAE: {mean_absolute_error(yte, pred_lgbm):.1f}")

    # SHAP (global importance focus)
    try:
        import shap
        explainer = shap.TreeExplainer(lgbm)
        shap_vals = explainer.shap_values(Xte)
        global_imp = pd.Series(np.abs(shap_vals).mean(0), index=Xte.columns).sort_values(ascending=False)
        print("\n[LightGBM] Mean |SHAP| (top features):")
        print(global_imp.head(10))
        print("\n[LightGBM] Route vs. Source/Destination contributions:")
        print(global_imp.loc[["route","source_city","destination_city"]])
    except Exception as e:
        print(f"[LightGBM] SHAP skipped: {e}")
except Exception as e:
    print(f"[LightGBM] skipped: {e}")

# ===========================================
# 5) MARS (Earth): piecewise effects of route/cities
# ===========================================
# Earth requires numeric inputs -> one-hot encode categoricals, keep numeric as-is.
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.pipeline import Pipeline
from sklearn.metrics import r2_score, mean_absolute_error
try:
    from pyearth import Earth  # sklearn-contrib-py-earth

    ohe = OneHotEncoder(handle_unknown="ignore", sparse_output=False)
    pre = ColumnTransformer([
        ("ohe", ohe, cat_cols),           # one-hot for categorical: includes route/source/destination
        ("pass", "passthrough", num_cols) # duration, days_left
    ])

    mars = Pipeline([
        ("pre", pre),
        ("earth", Earth(max_terms=50, max_degree=2))  # allow interactions up to 2
    ])
    mars.fit(X_train[cat_cols + num_cols], y_train)
    pred_mars = mars.predict(X_test[cat_cols + num_cols])
    print(f"\n[MARS] TEST R2: {r2_score(y_test, pred_mars):.3f} | MAE: {mean_absolute_error(y_test, pred_mars):.1f}")

    # Variable importance (grouped for route / source / destination)
    # Get names after one-hot:
    ohe_names = mars.named_steps["pre"].named_transformers_["ohe"].get_feature_names_out(cat_cols).tolist()
    feature_names = ohe_names + num_cols
    importances = mars.named_steps["earth"].feature_importances_
    imp_series = pd.Series(importances, index=feature_names).sort_values(ascending=False)

    # Group OHE columns back to original features
    def group_imp(prefix):
        cols = [c for c in imp_series.index if c.startswith(prefix + "_")]
        return float(imp_series.loc[cols].sum()) if cols else 0.0

    print("\n[MARS] Grouped importances (sum over one-hot columns):")
    for feat in ["route","source_city","destination_city","duration","days_left"]:
        print(f"{feat}: {group_imp(feat):.4f}")

except Exception as e:
    print(f"[MARS] skipped: {e}")

# ===========================================
# 6) Practical interpretation helpers
# ===========================================
print("\nINTERPRETATION TIPS:")
print("- Mixed Effects: compare variance components for source_city and destination_city; larger values imply")
print("  stronger city-specific price variability. Use the fixed-effect coefficients for duration/days_left/class controls.")
print("- CatBoost/LightGBM: if SHAP |route| (or source/destination) is large vs. other features, price materially changes by route/cities.")
print("- MARS: grouped importances for route/source/destination show piecewise impact; inspect which routes carry bigger effects.")

"""SPLINE AND LASSO"""

# === Setup ===
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer, make_column_selector as selector
from sklearn.preprocessing import OneHotEncoder, SplineTransformer, StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LassoCV
from sklearn.metrics import r2_score, mean_absolute_error

# Assumes df already has your columns + a constructed 'route'
# df["route"] = df["source_city"] + " -> " + df["destination_city"]

target = "price"
cat_cols = ["airline","source_city","departure_time","stops","arrival_time",
            "destination_city","class","route"]
num_cols = ["duration","days_left"]

df_ = df.dropna(subset=cat_cols + num_cols + [target]).copy()
X = df_[cat_cols + num_cols]
y = df_[target].values

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.30, random_state=42
)

# === Column transformer ===
# - One-hot for categoricals (route/source/destination included)
# - Spline bases for numeric features to capture piecewise effects
ohe = OneHotEncoder(handle_unknown="ignore", sparse_output=False)
spl = SplineTransformer(
    degree=3,           # cubic splines
    n_knots=7,          # more knots -> more flexibility (tune)
    include_bias=False
)

pre = ColumnTransformer(
    transformers=[
        ("cat", ohe, cat_cols),
        ("spline", spl, num_cols),
    ],
    remainder="drop"
)

# === Pipeline: Spline features + standardize + LassoCV ===
# LassoCV will select the amount of shrinkage, effectively choosing useful knots/levels
pipe = Pipeline([
    ("pre", pre),
    ("scale", StandardScaler(with_mean=False)),  # features may be sparse/dense mixed
    ("model", LassoCV(cv=5, random_state=42, n_alphas=100))
])

pipe.fit(X_train, y_train)
pred = pipe.predict(X_test)
print(f"[Spline+Lasso] TEST R2: {r2_score(y_test, pred):.3f} | MAE: {mean_absolute_error(y_test, pred):.1f}")

# === Inspect grouped importances ===
# Pull feature names after transform
ohe_names = pipe.named_steps["pre"].named_transformers_["cat"].get_feature_names_out(cat_cols).tolist()
spline_names = pipe.named_steps["pre"].named_transformers_["spline"].get_feature_names_out(num_cols).tolist()
feat_names = ohe_names + spline_names

coefs = pipe.named_steps["model"].coef_
coef_s = pd.Series(coefs, index=feat_names).sort_values(key=np.abs, ascending=False)

# Group by original high-level feature (sum of |coef| within its expanded columns)
def group_sum_abs(prefix):
    cols = [c for c in coef_s.index if c.startswith(prefix + "_")]
    return float(np.abs(coef_s.loc[cols]).sum()) if cols else 0.0

print("\n[Grouped |coef| (route / source / destination / spline blocks)]")
for feat in ["route","source_city","destination_city"]:
    print(f"{feat:16s}: {group_sum_abs(feat):.4f}")

# For spline blocks (numeric piecewise effects)
for f in num_cols:
    cols = [c for c in coef_s.index if c.startswith(f + "_bspl")]
    print(f"{f:16s}: {float(np.abs(coef_s.loc[cols]).sum() if cols else 0.0):.4f}")

# === Numeric partial effects for spline terms ===
# Build a small grid to visualize the learned piecewise shape (numerical printout)
def partial_effect_numeric(feature, grid=None):
    # Hold X at typical values; vary 'feature' across a grid
    X_base = X_train.iloc[:1].copy()
    # set reference levels for categoricals
    for c in cat_cols:
        X_base[c] = X_train[c].mode()[0]
    # set medians for numeric
    for c in num_cols:
        X_base[c] = float(X_train[c].median())

    if grid is None:
        lo, hi = np.percentile(X_train[feature], [5, 95])
        grid = np.linspace(lo, hi, 9)

    rows = []
    base_pred = pipe.predict(X_base)[0]
    for v in grid:
        Xv = X_base.copy()
        Xv[feature] = v
        yv = pipe.predict(Xv)[0]
        rows.append((float(v), float(yv - base_pred)))  # centered effect
    return pd.DataFrame(rows, columns=[feature, "effect_vs_baseline"])

for f in num_cols:
    print(f"\n[Numeric partial effect via splines] {f}")
    print(partial_effect_numeric(f).to_string(index=False))

# === Route-level effects (one-hot coefficients)
# Show the top route indicators by absolute coefficient
route_coefs = coef_s[coef_s.index.str.startswith("route_")]
print("\n[Top route one-hot coefficients by |coef|]:")
print(route_coefs.head(15))